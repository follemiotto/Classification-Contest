# -*- coding: utf-8 -*-
"""recipes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gE8PWPdss5mB8PA6cWHwgsOSXock-qqO
"""

!pip install tomotopy

import matplotlib.pyplot as plt
import wordcloud as wc
import pandas as pd
import spacy
import sys
import math
import tomotopy as tp
from gensim import corpora
from gensim.utils import simple_preprocess
from gensim.models import LdaModel
from gensim.models.coherencemodel import CoherenceModel
from gensim.models.phrases import Phrases, Phraser
ldamodel = tp.LDAModel
spacy.cli.download("en_core_web_md")
nlp = spacy.load('en_core_web_md')

runplace = True
number_of_documents=10000

if runplace:
  from google.colab import drive
  drive.mount('/content/drive')
  fpath='/content/drive/MyDrive/recipes/'
  receitas=pd.read_csv(fpath+'RecipeNLG_dataset.csv',nrows=number_of_documents)
else:
  receitas=pd.read_csv('RecipeNLG_dataset.csv',nrows=number_of_documents)

"""#Table Content"""

receitas.head()

receitas.columns

receitas2=receitas.drop(['Unnamed: 0','link','source','ingredients'],axis=1)
receitas2.head()

receitas2.info()

"""#Lemma and Token"""

docs=[]
for i in range(number_of_documents):
  aux=""
  aux+=receitas2['title'][i].lower().replace("("," ").replace(")"," ")+" "
  aux+=receitas2['NER'][i].lower().replace("[", "").replace("]", "").replace("\"", "")+" "
  aux+=receitas2['directions'][i].lower().replace("("," ").replace(")"," ").replace("[", "").replace("]", "").replace("\"", "")
  docs.append(aux)

docslemma=[]
len_raw=[]
print('Building lemmas...')
for i,d in enumerate(docs):
  print(i,end='')
  len_raw.append(len(d))
  tdoc=nlp(d)
  lm=" ".join([token.lemma_ for token in tdoc  if not(token.is_stop == True or token.is_digit == True or token.is_punct == True or '\\' in token.lemma_ or '/' in token.lemma_)])
  docslemma.append(lm)
  print('\r\r\r\r\r\r\r\r',end='')
len_lemma=[len(d) for d in docslemma]
print('# of characters (raw,pre): (%d,%d)'%(sum(len_raw),sum(len_lemma)))
print('Average # of characters (raw,pre): (%.2f,%.2f)'%(sum(len_raw)/len(len_raw),sum(len_lemma)/len(len_lemma)))

for i in docslemma[:10]:
  print(i)

k=[]
for i in docslemma:
  k.append(i.split())
lk=len(k)
shortest=9999
longest =0
average =0
for i in k:
  test=len(i)
  if test > longest:
    longest = test
  if test < shortest:
    shortest = test
  average+=test
average/=lk
print("""number of documents: {}
shortest doc: {}
longest doc : {}
average doc : {}
""".format(lk,shortest,longest,average))

for i in k[:10]:
  print(i)

st=""
for i in k:
  for j in i:
    st+=j+" "
mycloud = wc.WordCloud(max_words=4892,relative_scaling=1,width=1000,height=1000).generate(st)
plt.figure(figsize=(15,15))
plt.imshow(mycloud)

sts=st.split()
distinct=set(sts)
print("number of words: {}\nnumber of unique words: {}".format(len(sts),len(distinct)))

dtoken=[simple_preprocess(d, deacc= True, min_len=3) for d in docslemma]
phrases  = Phrases(dtoken, min_count = 2,threshold=9)
bigram=Phraser(phrases)
bdocs=[bigram[d] for d in dtoken]
bdc=[]
for i in bdocs:
  st=""
  for j in i:
    st+=j+" "
  bdc.append(st)
coll=[' '.join([d for w in l]) for l in bdocs]

for i in bdc[:10]:
  print(i)

for i in bdocs[:10]:
  print(i)

"""#BoW"""

col_tokenized=bdocs
dictionary=corpora.Dictionary()
BoW=[dictionary.doc2bow(doc, allow_update=True) for doc in col_tokenized]
print(BoW[:20])
[print([(dictionary[id], count) for id, count in line]) for line in BoW[:20]]

st=""
for i in bdc:
  st+=i+" "
mycloud = wc.WordCloud(max_words=4892,relative_scaling=1,width=1000,height=1000).generate(st)
plt.figure(figsize=(15,15))
plt.imshow(mycloud)

def printTopics(mdl,p=None):
	for k in range(mdl.k):
		print('Topic #{}'.format(k))
		if p is None:
			for word, prob in mdl.get_topic_words(topic_id=k,top_n=10):
				print(' ', word, prob, sep=' ')
		elif p==1:
			
			for word, prob in mdl.get_topic_words(topic_id=k,top_n=10,timepoint=0):
				print(' ', word, prob, sep=' ')
		else:
			for word, prob in mdl.get_topic_words(sub_topic_id=k,top_n=10):
				print(' ', word, prob, sep=' ')
			
			

def printCoherence(mdl):
	for preset in ('u_mass', 'c_uci', 'c_npmi', 'c_v'):
		coh = tp.coherence.Coherence(mdl, coherence=preset)
		average_coherence = coh.get_score()
		coherence_per_topic = [coh.get_score(topic_id=k) for k in range(mdl.k)]
	return average_coherence

def runModel(mdl,docs):
	for i,d in enumerate(docs):
		mdl.add_doc(d)
	mdl.burn_in = 100
	mdl.train(0)
	for i in range(0, 2000, 10):
		mdl.train(10)
    
	mdl.summary()
	mdl.save('test.lda.bin', True)

"""#Tests

##Attempt 1
"""

cv=[]
R=range(100,1000,150)
for i in R:
	mdl = tp.LDAModel(tw=tp.TermWeight.IDF, min_cf=3, rm_top=5, k=i,seed=7773) #,corpus=cp
	runModel(mdl,bdocs)
	cv.append(printCoherence(mdl))

plt.figure(figsize=(5,5))
plt.plot(list(R),cv,marker='o')
plt.grid(True)
plt.show()
print(max(zip(cv,list(R))))

"""##Attempt 2"""

cv=[]
R=range(200,300,15)
for i in R:
	mdl = tp.LDAModel(tw=tp.TermWeight.IDF, min_cf=3, rm_top=5, k=i,seed=7773)
	runModel(mdl,bdocs)
	cv.append(printCoherence(mdl))

plt.figure(figsize=(5,5))
plt.plot(list(R),cv,marker='o')
plt.grid(True)
plt.show()
print(max(zip(cv,list(R))))

"""##Attempt 3"""

cv=[]
R=range(180,220,5)
for i in R:
	mdl = tp.LDAModel(tw=tp.TermWeight.IDF, min_cf=3, rm_top=5, k=i,seed=7773)
	runModel(mdl,bdocs)
	cv.append(printCoherence(mdl))

plt.figure(figsize=(5,5))
plt.plot(list(R),cv,marker='o')
plt.grid(True)
plt.show()
print(max(zip(cv,list(R))))

"""##Results"""

mdl = tp.LDAModel(tw=tp.TermWeight.IDF, min_cf=3, rm_top=5, k=200,seed=7773)
runModel(mdl,bdocs)

"""##An Idea"""

cv=[]
R=range(2,100,5)
for i in R:
	mdl = tp.LDAModel(tw=tp.TermWeight.IDF, min_cf=3, rm_top=5, k=i,seed=7773)
	runModel(mdl,bdocs)
	cv.append(printCoherence(mdl)/i)

plt.figure(figsize=(5,5))
plt.plot(list(R),cv,marker='o')
plt.grid(True)
plt.show()
print(max(zip(cv,list(R))))

cv=[]
R=range(70,90,1)
for i in R:
	mdl = tp.LDAModel(tw=tp.TermWeight.IDF, min_cf=3, rm_top=5, k=i,seed=7773)
	runModel(mdl,bdocs)
	cv.append(printCoherence(mdl))

plt.figure(figsize=(5,5))
plt.plot(list(R),cv,marker='o')
plt.grid(True)
plt.show()
print(max(zip(cv,list(R))))

"""##True Result"""

cv=[]
R=range(900,1000,99)
for i in R:
	mdl = tp.LDAModel(tw=tp.TermWeight.IDF, min_cf=3, rm_top=5, k=i,seed=7773)
	runModel(mdl,bdocs)
	cv.append(printCoherence(mdl))

plt.figure(figsize=(5,5))
plt.plot(list(R),cv,marker='o')
plt.grid(True)
plt.show()
print(max(zip(cv,list(R))))

cv=[]
R=range(800,1000,49)
for i in R:
	mdl = tp.LDAModel(tw=tp.TermWeight.IDF, min_cf=3, rm_top=5, k=i,seed=7773)
	runModel(mdl,bdocs)
	cv.append(printCoherence(mdl))

plt.figure(figsize=(5,5))
plt.plot(list(R),cv,marker='o')
plt.grid(True)
plt.show()
print(max(zip(cv,list(R))))

"""#Model"""

myModel = LdaModel(corpus = BoW,num_topics= 74, random_state= 27644437, id2word = dictionary, alpha = 'auto',per_word_topics = True,passes = 100)
for t in myModel.show_topics():
  print(t)

# docs topics
for i,d in enumerate(BoW): # collection and bagofwords must be synchronized
  print(bdocs[i],':',myModel.get_document_topics(d,minimum_probability=0.3)) #threshold

tops=[]
for k in range(2):
    t=[]
    for wt in myModel.show_topic(k):
      t.append(wt[0])
    tops.append(t)
for t in tops:
  topic_coher=CoherenceModel(topics=[t], texts= bdocs, dictionary=dictionary, coherence='c_v')
  print('topic:',t)
  print('C_v:%.3f'%(topic_coher.get_coherence()))
print("All:")
topic_coher=CoherenceModel(topics=tops, texts= bdocs, dictionary=dictionary, coherence='c_v')
print('C_v:%.3f'%(topic_coher.get_coherence()))
# top topics
print('Topics with the highest coherence score the coherence for each topic.')
myModel.top_topics(corpus=BoW,dictionary=dictionary,coherence='c_v',texts=bdocs,topn=5)

tops_frequency=[0]*74
x=0
for d in BoW:
  aux=myModel.get_document_topics(d,minimum_probability=0.5)
  if aux!=[]:
    tops_frequency[aux[0][0]]+=1
print(tops_frequency)
  # print(myModel.get_document_topics(d,minimum_probability=0.3)) #threshold
for i in range(74):
  aux1=[tops_frequency[i],i]
  tops_frequency[i]=aux1
tops_frequency.sort()
tops_frequency.reverse()
print(tops_frequency)
aux2=[0]*10
for i in range(10):
  aux2[i]=tops_frequency[i][1]
print(aux2)

for i in aux2:
  print(myModel.show_topics(num_topics=74)[i])

myModel.top_topics(corpus=BoW,dictionary=dictionary,coherence='c_v',texts=bdocs,topn=10)[:10]

# aux2=[56, 27, 31, 37, 19, 34, 44, 62, 47, 25]
aux2+=[33,16,46,58,40]
for i in aux2:
  print(myModel.show_topics(num_topics=74)[i])

"""#Labeling"""

ddd=[]
for j in aux2:
  ddd2=[]
  for i,d in enumerate(BoW):
    if len(ddd)<10:
      aux=myModel.get_document_topics(d,minimum_probability=0.5)
    else:
      aux=myModel.get_document_topics(d,minimum_probability=0.3)
    if aux!=[]:
      if aux[0][0]==j:
        ddd2.append(i)
        # print(aux[0],':',i,bdocs[i])
  ddd.append(ddd2)

for i in ddd:
  print(len(i),end=" ")

receitas.head()

"""##seeing all the recipes per topic"""

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', 1)

lables=[]

"""###topic 0"""

print(myModel.show_topics(num_topics=74)[aux2[0]])

receitasaux=receitas.iloc[ddd[0]]
receitasaux.head(82)

lables.append("Sweet breads: Breads, Cakes and Muffins")

"""###topic 1"""

print(myModel.show_topics(num_topics=74)[aux2[1]])

receitasaux=receitas.iloc[ddd[1]]
receitasaux.head(82)

lables.append("Hard dough: Cookies, Pies and Brownies")

"""###topic 2"""

print(myModel.show_topics(num_topics=74)[aux2[2]])

receitasaux=receitas.iloc[ddd[2]]
receitasaux.head(82)

lables.append("Meat lunch: Beans, Steak, Pasta and Sauces")

"""###topic 3"""

print(myModel.show_topics(num_topics=74)[aux2[3]])

receitasaux=receitas.iloc[ddd[3]]
receitasaux.head(82)

lables.append("Pound Cakes")

"""###topic 4"""

print(myModel.show_topics(num_topics=74)[aux2[4]])

receitasaux=receitas.iloc[ddd[4]]
receitasaux.head(82)

lables.append("Dough: Bread, Rolls, Buns and Pizza dough")

"""###topic 5"""

print(myModel.show_topics(num_topics=74)[aux2[5]])

receitasaux=receitas.iloc[ddd[5]]
receitasaux.head(82)

lables.append("Slaw and Salads")

"""###topic 6"""

print(myModel.show_topics(num_topics=74)[aux2[6]])

receitasaux=receitas.iloc[ddd[6]]
receitasaux.head(82)

lables.append("Fruit Salad")

"""###topic 7"""

print(myModel.show_topics(num_topics=74)[aux2[7]])

receitasaux=receitas.iloc[ddd[7]]
receitasaux.head(82)

lables.append("Baked Snacks")

"""###topic 8"""

print(myModel.show_topics(num_topics=74)[aux2[8]])

receitasaux=receitas.iloc[ddd[8]]
receitasaux.head(82)

lables.append("Soup")

"""###topic 9"""

print(myModel.show_topics(num_topics=74)[aux2[9]])

receitasaux=receitas.iloc[ddd[9]]
receitasaux.head(82)

lables.append("Casserole")

"""###topic 10"""

print(myModel.show_topics(num_topics=74)[aux2[10]])

receitasaux=receitas.iloc[ddd[10]]
receitasaux.head(82)

lables.append("Bars, Snacks and other Desserts")

"""###topic 11"""

print(myModel.show_topics(num_topics=74)[aux2[11]])

receitasaux=receitas.iloc[ddd[11]]
receitasaux.head(82)

lables.append("Chili dishes and Pastas")

"""###topic 12"""

print(myModel.show_topics(num_topics=74)[aux2[12]])

receitasaux=receitas.iloc[ddd[12]]
receitasaux.head(82)

lables.append("Baked dishes with dry seeds")

"""###topic 13"""

print(myModel.show_topics(num_topics=74)[aux2[13]])

receitasaux=receitas.iloc[ddd[13]]
receitasaux.head(82)

lables.append("Pizzas, Lasagnas and other dishes with Cheese")

"""###topic 14"""

print(myModel.show_topics(num_topics=74)[aux2[14]])

receitasaux=receitas.iloc[ddd[14]]
receitasaux.head(82)

lables.append("Soft desserts: Ice Cream, Pudding and others")

"""#Result"""

for i,j in zip(aux2,lables):
  print(j,"\n",myModel.show_topics(num_topics=74)[i])