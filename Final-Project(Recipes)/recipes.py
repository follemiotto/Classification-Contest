# -*- coding: utf-8 -*-
"""recipes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gE8PWPdss5mB8PA6cWHwgsOSXock-qqO
"""

# from google.colab import drive
# drive.mount('/content/drive')
# fpath='/content/drive/MyDrive/recipes/'

!pip install tomotopy

import matplotlib.pyplot as plt
import wordcloud as wc
import pandas as pd
import spacy
import sys
import math
import tomotopy as tp
from gensim import corpora
from gensim.utils import simple_preprocess
from gensim.models import LdaModel
from gensim.models.coherencemodel import CoherenceModel
from gensim.models.phrases import Phrases, Phraser
ldamodel = tp.LDAModel
spacy.cli.download("en_core_web_md")
nlp = spacy.load('en_core_web_md')

# frec = open(fpath+'RecipeNLG_dataset.csv')
# rec=frec.readlines()
number_of_documents=10000
# receitas=pd.read_csv(fpath+'RecipeNLG_dataset.csv',nrows=number_of_documents)
receitas=pd.read_csv('RecipeNLG_dataset.csv',nrows=number_of_documents)

# for i in range(len(rec['title'])):
#   if 'bible' in str(rec['title'][i]).lower():
#     print(rec['title'][i])
#     print(rec['ingredients'][i])
#     print(rec['directions'][i])
#     print()
#     print()
receitas.head()

receitas.columns

receitas.drop(['Unnamed: 0','link','source','ingredients'],axis=1,inplace=True)
receitas.head()

receitas.info()

docs=[]
for i in range(number_of_documents):
  aux=""
  aux+=receitas['title'][i].lower().replace("("," ").replace(")"," ")+" "
  aux+=receitas['NER'][i].lower().replace("[", "").replace("]", "").replace("\"", "")+" "
  # aux+=receitas['ingredients'][i].lower().replace("("," ").replace(")"," ").replace("[", "").replace("]", "").replace("\"", "").replace("tsp","tea_spoon").replace("tbsp","soup_spoon").replace("c.","cup").replace("oz","ounce").replace("lb","pound").replace("pkg","package")+" "
  aux+=receitas['directions'][i].lower().replace("("," ").replace(")"," ").replace("[", "").replace("]", "").replace("\"", "")
  docs.append(aux)

docslemma=[]
len_raw=[]
print('Building lemmas...')
for i,d in enumerate(docs):
  print(i,end='')
  len_raw.append(len(d))
  tdoc=nlp(d)
  lm=" ".join([token.lemma_ for token in tdoc  if not(token.is_stop == True or token.is_digit == True or token.is_punct == True or '\\' in token.lemma_ or '/' in token.lemma_)])
  docslemma.append(lm)
  print('\r\r\r\r\r\r\r\r',end='')
len_lemma=[len(d) for d in docslemma]
print('# of characters (raw,pre): (%d,%d)'%(sum(len_raw),sum(len_lemma)))
print('Average # of characters (raw,pre): (%.2f,%.2f)'%(sum(len_raw)/len(len_raw),sum(len_lemma)/len(len_lemma)))

for i in docslemma[:10]:
  print(i)

k=[]
for i in docslemma:
  k.append(i.split())
lk=len(k)
shortest=9999
longest =0
average =0
for i in k:
  test=len(i)
  if test > longest:
    longest = test
  if test < shortest:
    shortest = test
  average+=test
average/=lk
print("""number of documents: {}
shortest doc: {}
longest doc : {}
average doc : {}
""".format(lk,shortest,longest,average))

for i in k[:10]:
  print(i)

plt.figure(figsize=(15,15))

st=""
for i in k:
  for j in i:
    st+=j+" "
mycloud = wc.WordCloud().generate(st)
plt.imshow(mycloud)

sts=st.split()
distinct=set(sts)
print("number of words: {}\nnumber of unique words: {}".format(len(sts),len(distinct)))

dtoken=[simple_preprocess(d, deacc= True, min_len=3) for d in docslemma]
phrases  = Phrases(dtoken, min_count = 2,threshold=9)
bigram=Phraser(phrases)
bdocs=[bigram[d] for d in dtoken]
bdc=[]
for i in bdocs:
  st=""
  for j in i:
    st+=j+" "
  bdc.append(st)
coll=[' '.join([d for w in l]) for l in bdocs]

for i in bdc[:10]:
  print(i)

for i in bdocs[:10]:
  print(i)

col_tokenized=bdocs
dictionary=corpora.Dictionary()
BoW=[dictionary.doc2bow(doc, allow_update=True) for doc in col_tokenized]
print(BoW[:20])
[print([(dictionary[id], count) for id, count in line]) for line in BoW[:20]]

plt.figure(figsize=(15,15))

st=""
for i in bdc:
  st+=i+" "
mycloud = wc.WordCloud().generate(st)
plt.imshow(mycloud)

def printTopics(mdl,p=None):
	for k in range(mdl.k):
		print('Topic #{}'.format(k))
		if p is None:
			for word, prob in mdl.get_topic_words(topic_id=k,top_n=10):
				print(' ', word, prob, sep=' ')
		elif p==1:
			
			for word, prob in mdl.get_topic_words(topic_id=k,top_n=10,timepoint=0):
				print(' ', word, prob, sep=' ')
		else:
			for word, prob in mdl.get_topic_words(sub_topic_id=k,top_n=10):
				print(' ', word, prob, sep=' ')
			
			

def printCoherence(mdl):
	for preset in ('u_mass', 'c_uci', 'c_npmi', 'c_v'):
		coh = tp.coherence.Coherence(mdl, coherence=preset)
		average_coherence = coh.get_score()
		coherence_per_topic = [coh.get_score(topic_id=k) for k in range(mdl.k)]
		# print('==== Coherence : {} ===='.format(preset))
		# print('Average:', average_coherence, '\nPer Topic:', coherence_per_topic)
		# print()
	return average_coherence

def runModel(mdl,docs):
	for i,d in enumerate(docs):
		# print(i,end='')
		#ch = d.split()
		mdl.add_doc(d)
		# print('\r\r\r\r\r\r\r\r\r\r',end='')
	# print()
	mdl.burn_in = 100
	mdl.train(0)
	# print('Num docs:', len(mdl.docs), ', Vocab size:', len(mdl.used_vocabs), ', Num words:', mdl.num_words)
	# print('Removed top words:', mdl.removed_top_words)
	# print('Training...', file=sys.stderr, flush=True)
	for i in range(0, 2000, 10):
		mdl.train(10)
		# print('Iteration: {}\tLog-likelihood: {}'.format(i, mdl.ll_per_word))
    
	mdl.summary()
	# print('Saving...', file=sys.stderr, flush=True)
	mdl.save('test.lda.bin', True)

"""#Attempt 1"""

cv=[]
R=range(100,1000,150)
# model=1
# if model==1 or model==0:
for i in R:
	# print('************************** Running LDA ***************************')
	#cp=tp.utils.Corpus(tokenizer=tp.utils.SimpleTokenizer()) #tokenizer=tp.utils.SimpleTokenizer())
	#cp.process(coll)
	mdl = tp.LDAModel(tw=tp.TermWeight.IDF, min_cf=3, rm_top=5, k=i,seed=7773) #,corpus=cp
	runModel(mdl,bdocs)
	#    for n, line in enumerate(open(input_file, encoding='utf-8')):
	# printTopics(mdl)
	cv.append(printCoherence(mdl))

# if model==2 or model==0:
# 	print('************************** Running DMR ***************************')
# 	mdl = tp.DMRModel(tw=tp.TermWeight.IDF, min_cf=3, rm_top=5, k=10,seed=7773) #,corpus=cp
# 	runModel(mdl)
# 	printTopics(mdl)













# 	printCoherence(mdl)

# if model==3 or model==0:
# 	print('************************** Running DTM ***************************')
# 	mdl = tp.DTModel(tw=tp.TermWeight.IDF, min_cf=3, rm_top=5, k=10,seed=7773) #,corpus=cp
# 	runModel(mdl)
# 	printTopics(mdl,1)
# 	#printCoherence(mdl)

# if model==4 or model==0:
# 	print('************************** Running CTM ***************************')
# 	mdl = tp.CTModel(tw=tp.TermWeight.IDF, min_cf=3, rm_top=5, k=10)
# 	runModel(mdl)
# 	printTopics(mdl)
# 	printCoherence(mdl)
	
# if model==5 or model==0:
# 	print('************************** Running HDP ***************************')
# 	mdl = tp.HDPModel(tw=tp.TermWeight.IDF, min_cf=3, rm_top=5, initial_k=10)
# 	runModel(mdl)
# 	printTopics(mdl)
# 	printCoherence(mdl)

# if model==6 or model==0:
# 	print('************************** Running HLDA ***************************')
# 	mdl = tp.HLDAModel(tw=tp.TermWeight.IDF, min_cf=3, rm_top=5)
# 	runModel(mdl)
# 	printTopics(mdl)
# 	printCoherence(mdl)

# if model==7 or model==0:
# 	print('************************** Running MGLDA ***************************')
# 	mdl = tp.MGLDAModel (tw=tp.TermWeight.IDF, min_cf=3, rm_top=5,k_g=10,k_l=10)
# 	runModel(mdl)
# 	printTopics(mdl)
# 	printCoherence(mdl)

# if model==8 or model==0:
# 	print('************************** Running PA ***************************')
# 	mdl = tp.PAModel (tw=tp.TermWeight.IDF, min_cf=3, rm_top=5,k1=10,k2=10)
# 	runModel(mdl)
# 	printTopics(mdl,2)
# 	printCoherence(mdl)

# if model==9 or model==0:
# 	print('************************** Running HPA ***************************')
# 	mdl = tp.HPAModel (tw=tp.TermWeight.IDF, min_cf=3, rm_top=5,k1=10,k2=10)
# 	runModel(mdl)
# 	printTopics(mdl)
# 	printCoherence(mdl)

plt.figure(figsize=(5,5))
plt.plot(list(R),cv,marker='o')
plt.grid(True)
plt.show()
print(max(zip(cv,list(R))))

"""#Attempt 2"""

cv=[]
R=range(200,300,15)
for i in R:
	mdl = tp.LDAModel(tw=tp.TermWeight.IDF, min_cf=3, rm_top=5, k=i,seed=7773)
	runModel(mdl,bdocs)
	cv.append(printCoherence(mdl))

plt.figure(figsize=(5,5))
plt.plot(list(R),cv,marker='o')
plt.grid(True)
plt.show()
print(max(zip(cv,list(R))))

"""#Attempt 3"""

cv=[]
R=range(180,220,5)
for i in R:
	mdl = tp.LDAModel(tw=tp.TermWeight.IDF, min_cf=3, rm_top=5, k=i,seed=7773)
	runModel(mdl,bdocs)
	cv.append(printCoherence(mdl))

plt.figure(figsize=(5,5))
plt.plot(list(R),cv,marker='o')
plt.grid(True)
plt.show()
print(max(zip(cv,list(R))))

"""#Results"""

mdl = tp.LDAModel(tw=tp.TermWeight.IDF, min_cf=3, rm_top=5, k=200,seed=7773)
runModel(mdl,bdocs)

"""#A Idea"""

cv=[]
R=range(2,100,5)
for i in R:
	mdl = tp.LDAModel(tw=tp.TermWeight.IDF, min_cf=3, rm_top=5, k=i,seed=7773)
	runModel(mdl,bdocs)
	cv.append(printCoherence(mdl)/i)

plt.figure(figsize=(5,5))
plt.plot(list(R),cv,marker='o')
plt.grid(True)
plt.show()
print(max(zip(cv,list(R))))

cv=[]
R=range(70,90,1)
for i in R:
	mdl = tp.LDAModel(tw=tp.TermWeight.IDF, min_cf=3, rm_top=5, k=i,seed=7773)
	runModel(mdl,bdocs)
	cv.append(printCoherence(mdl))

plt.figure(figsize=(5,5))
plt.plot(list(R),cv,marker='o')
plt.grid(True)
plt.show()
print(max(zip(cv,list(R))))

"""#Model"""

from gensim.models import LdaModel
myModel = LdaModel(corpus = BoW,num_topics= 74, random_state= 27644437, id2word = dictionary, alpha = 'auto',per_word_topics = True,passes = 100)
for t in myModel.show_topics():
  print(t)

# docs topics
for i,d in enumerate(BoW): # collection and bagofwords must be synchronized
  print(bdocs[i],':',myModel.get_document_topics(d,minimum_probability=0.3)) #threshold

tops=[]
for k in range(2):
    t=[]
    for wt in myModel.show_topic(k):
      t.append(wt[0])
    tops.append(t)
for t in tops:
  topic_coher=CoherenceModel(topics=[t], texts= bdocs, dictionary=dictionary, coherence='c_v')
  print('topic:',t)
  print('C_v:%.3f'%(topic_coher.get_coherence()))
print("All:")
topic_coher=CoherenceModel(topics=tops, texts= bdocs, dictionary=dictionary, coherence='c_v')
print('C_v:%.3f'%(topic_coher.get_coherence()))
# top topics
print('Topics with the highest coherence score the coherence for each topic.')
myModel.top_topics(corpus=BoW,dictionary=dictionary,coherence='c_v',texts=bdocs,topn=5)